# Bose Professional Technical Assistant
### AI-Powered RAG System for Technical Documentation

---

## What This System Does

This is an **intelligent Q&A system** that allows users to ask technical questions about Bose Professional products in natural language and receive instant, accurate answers with source citations.

**The Problem It Solves:**
- Engineers and technicians need quick answers from hundreds of pages of technical documentation
- Searching through multiple PDF manuals is time-consuming and frustrating
- Critical information is buried in dense technical documents

**The Solution:**
- Ask questions in plain English: *"What is the maximum number of analog inputs on the EX-1280C?"*
- Get accurate answers in 2-3 seconds with page references
- 100% local processing (no cloud APIs, complete privacy)
- Professional web interface matching Bose brand standards

**End-User Experience:**
1. User opens web browser to http://localhost:8000
2. Types a question in natural language (no special syntax needed)
3. System searches all processed documents using AI-powered semantic search
4. Receives clear answer with source page numbers in under 3 seconds
5. Can ask follow-up questions or explore related topics

---

## Project Structure Explained

**Understanding the Architecture:** This project follows a modular RAG (Retrieval-Augmented Generation) pipeline with clear separation of concerns. Here's how the codebase is organized:

```
bose-rag-project/
â”‚
â”œâ”€â”€ app.py                            # FastAPI application - Production web server
â”‚   â””â”€> Serves the web UI and REST API endpoints
â”‚   â””â”€> Initializes RAG system on startup
â”‚   â””â”€> Handles /api/query, /api/health, /api/info routes
â”‚
â”œâ”€â”€ launch_fastapi.bat                # Convenience launcher with pre-flight checks
â”‚
â”œâ”€â”€ requirements.txt                  # All Python dependencies with versions
â”‚
â”œâ”€â”€ config/                           # CONFIGURATION LAYER
â”‚   â”œâ”€â”€ settings.py                   # Environment variables (.env) loader
â”‚   â”‚   â””â”€> OLLAMA_BASE_URL, EMBEDDING_MODEL, CHUNK_SIZE, etc.
â”‚   â”‚
â”‚   â””â”€â”€ constants.py                  # System-wide constants and enums
â”‚       â””â”€> ContentType enum (SPECIFICATION, PROCEDURE, GENERAL)
â”‚       â””â”€> ErrorType enum for structured error handling
â”‚       â””â”€> Default values (CHUNK_SIZE=800, CHUNK_OVERLAP=100)
â”‚
â”œâ”€â”€ data/                             # DATA STORAGE
â”‚   â”œâ”€â”€ vector_db/                    # ChromaDB persistent storage (auto-created)
â”‚   â”‚   â””â”€> Stores document embeddings and metadata
â”‚   â”‚   â””â”€> Survives application restarts
â”‚   â”‚
â”‚   â””â”€â”€ *.pdf                         # Source PDF documents (place here)
â”‚
â”œâ”€â”€ src/                              # CORE APPLICATION CODE
â”‚   â”‚
â”‚   â”œâ”€â”€ content_detection/            # CONTENT CLASSIFICATION
â”‚   â”‚   â””â”€â”€ detector.py               
â”‚   â”‚       â””â”€> Analyzes text to determine if it's:
â”‚   â”‚           â€¢ Specification (technical data, numbers, specs)
â”‚   â”‚           â€¢ Procedure (installation, setup, how-to)
â”‚   â”‚           â€¢ General (overview, features, descriptions)
â”‚   â”‚       â””â”€> Uses keyword matching and pattern recognition
â”‚   â”‚       â””â”€> Helps build appropriate prompts for LLM
â”‚   â”‚
â”‚   â”œâ”€â”€ document_processing/          # PDF PROCESSING PIPELINE
â”‚   â”‚   â”œâ”€â”€ base_processor.py        # Abstract base class defining interface
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ text_processor.py        # Standard text extraction (PyPDF2)
â”‚   â”‚   â”‚   â””â”€> Handles most PDFs with selectable text
â”‚   â”‚   â”‚   â””â”€> Fast, primary extraction method
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ table_processor.py       # Table extraction (Camelot)
â”‚   â”‚   â”‚   â””â”€> Detects and extracts tabular data
â”‚   â”‚   â”‚   â””â”€> Preserves structure of spec tables
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ image_processor.py       # OCR processing (Tesseract)
â”‚   â”‚   â”‚   â””â”€> Handles scanned PDFs or image-based pages
â”‚   â”‚   â”‚   â””â”€> Fallback when text extraction fails
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ router.py                # Processing orchestrator
â”‚   â”‚       â””â”€> Decides which processor to use per page
â”‚   â”‚       â””â”€> Tries text â†’ table â†’ OCR in that order
â”‚   â”‚       â””â”€> Chunks extracted text into semantic segments
â”‚   â”‚
â”‚   â”œâ”€â”€ embeddings/                   # VECTOR EMBEDDINGS
â”‚   â”‚   â””â”€â”€ embedder.py               
â”‚   â”‚       â””â”€> Wraps HuggingFace sentence-transformers
â”‚   â”‚       â””â”€> Model: all-MiniLM-L6-v2 (lightweight, fast)
â”‚   â”‚       â””â”€> Converts text to 384-dimensional vectors
â”‚   â”‚       â””â”€> Enables semantic similarity search
â”‚   â”‚
â”‚   â”œâ”€â”€ vector_store/                 # VECTOR DATABASE
â”‚   â”‚   â””â”€â”€ chromadb_manager.py      
â”‚   â”‚       â””â”€> Manages ChromaDB operations
â”‚   â”‚       â””â”€> PersistentClient (data survives restarts)
â”‚   â”‚       â””â”€> Stores: embeddings + metadata (page, source, content_type)
â”‚   â”‚       â””â”€> add_documents(): Batch insert with unique IDs
â”‚   â”‚       â””â”€> search(): Cosine similarity search, returns top-k
â”‚   â”‚
â”‚   â”œâ”€â”€ retrieval/                    # DOCUMENT RETRIEVAL
â”‚   â”‚   â””â”€â”€ content_aware_retriever.py
â”‚   â”‚       â””â”€> Queries vector store for relevant chunks
â”‚   â”‚       â””â”€> Retrieves top-k (default k=5) most similar documents
â”‚   â”‚       â””â”€> Returns Document objects with metadata preserved
â”‚   â”‚       â””â”€> Maintains original ranking from ChromaDB
â”‚   â”‚
â”‚   â”œâ”€â”€ generation/                   # ANSWER GENERATION
â”‚   â”‚   â”œâ”€â”€ llm_handler_phi.py       # Ollama + Phi-2 integration
â”‚   â”‚   â”‚   â””â”€> Connects to local Ollama server
â”‚   â”‚   â”‚   â””â”€> Uses Phi-2 model (1.6GB, fast, local)
â”‚   â”‚   â”‚   â””â”€> generate(): Sends prompt, gets answer
â”‚   â”‚   â”‚   â””â”€> Configurable max_tokens, temperature
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ prompt_builder.py        # Context-aware prompt construction
â”‚   â”‚       â””â”€> Detects query intent (spec/procedure/general)
â”‚   â”‚       â””â”€> Builds appropriate prompt template
â”‚   â”‚       â””â”€> Includes retrieved context + user question
â”‚   â”‚       â””â”€> Optimizes for concise, accurate answers
â”‚   â”‚
â”‚   â”œâ”€â”€ interfaces/                   # USER INTERFACES
â”‚   â”‚   â”œâ”€â”€ rag_phi.py               # Main RAG orchestrator (BoseRAGPhi class)
â”‚   â”‚   â”‚   â””â”€> Coordinates all components
â”‚   â”‚   â”‚   â””â”€> process_documents(): Ingests PDFs â†’ chunks â†’ embeddings â†’ DB
â”‚   â”‚   â”‚   â””â”€> answer_query(): Full RAG pipeline
â”‚   â”‚   â”‚   â””â”€> Handles errors gracefully, logs everything
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ gradio_app.py            # Alternative Gradio UI (simple demo)
â”‚   â”‚
â”‚   â””â”€â”€ error_handling/               # LOGGING & ERROR MANAGEMENT
â”‚       â”œâ”€â”€ logger.py                 # Rotating file logger
â”‚       â”‚   â””â”€> Writes to rag_system.log
â”‚       â”‚   â””â”€> Max 10MB per file, keeps 5 backups
â”‚       â”‚   â””â”€> Console + file output
â”‚       â”‚
â”‚       â””â”€â”€ handlers.py               # Structured error handling
â”‚           â””â”€> ErrorType enum categorization
â”‚           â””â”€> Collects error statistics
â”‚           â””â”€> Provides error summaries
â”‚
â”œâ”€â”€ static/                           # FRONTEND (FastAPI)
â”‚   â”œâ”€â”€ index.html                    # Main web interface
â”‚   â”‚   â””â”€> Professional Bose-branded UI
â”‚   â”‚   â””â”€> Chat-style message interface
â”‚   â”‚   â””â”€> Real-time status indicators
â”‚   â”‚
â”‚   â”œâ”€â”€ styles.css                    # Bose Professional dark theme
â”‚   â”‚   â””â”€> CSS variables for easy customization
â”‚   â”‚   â””â”€> --bose-accent: #00a0dc (brand color)
â”‚   â”‚   â””â”€> Responsive design (mobile-friendly)
â”‚   â”‚
â”‚   â””â”€â”€ app.js                        # Frontend JavaScript logic
â”‚       â””â”€> API calls to /api/query
â”‚       â””â”€> Message rendering and chat history
â”‚       â””â”€> Loading states and error handling
â”‚
â”œâ”€â”€ scripts/                          # UTILITY SCRIPTS
â”‚   â”œâ”€â”€ demo.py                       # Document processing demo
â”‚   â”‚   â””â”€> Run this first to populate database
â”‚   â”‚   â””â”€> Processes PDFs in data/ folder
â”‚   â”‚   â””â”€> Shows progress and statistics
â”‚   â”‚
â”‚   â”œâ”€â”€ check_db.py                   # Database inspection tool
â”‚   â”‚   â””â”€> Shows document count
â”‚   â”‚   â””â”€> Tests sample queries
â”‚   â”‚   â””â”€> Displays top results with distances
â”‚   â”‚
â”‚   â”œâ”€â”€ clear_db.py                   # Database reset utility
â”‚   â”‚   â””â”€> Deletes vector_db directory
â”‚   â”‚   â””â”€> Use when reprocessing documents
â”‚   â”‚
â”‚   â””â”€â”€ launch_app.py                 # Gradio launcher with checks
â”‚
â””â”€â”€ tests/                            # UNIT TESTS (future)
```

### Key Design Principles

1. **Modularity**: Each component has a single responsibility
2. **Separation of Concerns**: Processing â†’ Storage â†’ Retrieval â†’ Generation are independent
3. **Error Resilience**: Comprehensive error handling at every layer
4. **Configurability**: Environment variables for all settings
5. **Persistence**: Vector DB survives restarts, no reprocessing needed
6. **Observability**: Detailed logging for debugging and monitoring

---

## System Architecture & Data Flow

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          END USER                                    â”‚
â”‚              (Opens browser, types question)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      WEB INTERFACE                                   â”‚
â”‚                   (FastAPI + HTML/CSS/JS)                            â”‚
â”‚                                                                       â”‚
â”‚  â€¢ Professional Bose-branded UI                                      â”‚
â”‚  â€¢ Real-time status display                                          â”‚
â”‚  â€¢ Chat-style message interface                                      â”‚
â”‚  â€¢ REST API endpoints (/api/query, /api/health)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAG ORCHESTRATOR                                  â”‚
â”‚                    (BoseRAGPhi Class)                                â”‚
â”‚                                                                       â”‚
â”‚  â€¢ Receives user question                                            â”‚
â”‚  â€¢ Coordinates all components                                        â”‚
â”‚  â€¢ Handles errors gracefully                                         â”‚
â”‚  â€¢ Returns formatted response                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                                      â”‚
       â”‚ ONE-TIME SETUP PHASE                    QUERY PHASE (Every Request)
       â”‚                                                      â”‚
       â–¼                                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DOCUMENT INGESTION  â”‚                    â”‚   QUERY PROCESSING       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                                      â”‚
       â”‚                                                      â”‚
       â–¼                                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. PDF Processing   â”‚                    â”‚  1. Question Embedding   â”‚
â”‚     Router           â”‚                    â”‚     (all-MiniLM-L6-v2)   â”‚
â”‚                      â”‚                    â”‚                          â”‚
â”‚  Tries in order:     â”‚                    â”‚  "What is max inputs?"   â”‚
â”‚  â€¢ Text extraction   â”‚                    â”‚          â†“               â”‚
â”‚  â€¢ Table extraction  â”‚                    â”‚  [0.23, -0.45, 0.78...] â”‚
â”‚  â€¢ OCR (fallback)    â”‚                    â”‚  (384-dim vector)        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                               â”‚
       â–¼                                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Content          â”‚                    â”‚  2. Vector Search        â”‚
â”‚     Detection        â”‚                    â”‚     (ChromaDB)           â”‚
â”‚                      â”‚                    â”‚                          â”‚
â”‚  Classifies as:      â”‚                    â”‚  Searches 156 docs       â”‚
â”‚  â€¢ Specification     â”‚                    â”‚  Finds top 5 similar     â”‚
â”‚  â€¢ Procedure         â”‚                    â”‚  Returns with metadata   â”‚
â”‚  â€¢ General           â”‚                    â”‚                          â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                               â”‚
       â–¼                                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Text Chunking    â”‚                    â”‚  3. Context Retrieval    â”‚
â”‚                      â”‚                    â”‚                          â”‚
â”‚  Splits into         â”‚                    â”‚  Retrieved Documents:    â”‚
â”‚  semantic chunks     â”‚                    â”‚  â€¢ Chunk 1 (Page 12)     â”‚
â”‚  (800 chars,         â”‚                    â”‚  â€¢ Chunk 2 (Page 45)     â”‚
â”‚   100 overlap)       â”‚                    â”‚  â€¢ Chunk 3 (Page 23)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚  â€¢ Chunk 4 (Page 67)     â”‚
       â”‚                                    â”‚  â€¢ Chunk 5 (Page 34)     â”‚
       â–¼                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚  4. Generate         â”‚                               â–¼
â”‚     Embeddings       â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      â”‚                    â”‚  4. Prompt Construction  â”‚
â”‚  Convert each chunk  â”‚                    â”‚     (Context-Aware)      â”‚
â”‚  to vector           â”‚                    â”‚                          â”‚
â”‚  (all-MiniLM-L6-v2)  â”‚                    â”‚  Builds prompt with:     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚  â€¢ Query intent detected â”‚
       â”‚                                    â”‚  â€¢ Retrieved context     â”‚
       â–¼                                    â”‚  â€¢ User question         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚  â€¢ Answer instructions   â”‚
â”‚  5. Store in         â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚     Vector DB        â”‚                               â”‚
â”‚                      â”‚                               â–¼
â”‚  ChromaDB:           â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â€¢ Embeddings        â”‚                    â”‚  5. LLM Generation       â”‚
â”‚  â€¢ Metadata          â”‚                    â”‚     (Phi-2 via Ollama)   â”‚
â”‚    - Page number     â”‚                    â”‚                          â”‚
â”‚    - Source file     â”‚                    â”‚  Prompt â†’ Ollama API     â”‚
â”‚    - Content type    â”‚                    â”‚  Phi-2 processes         â”‚
â”‚  â€¢ Persistent        â”‚                    â”‚  Generates answer        â”‚
â”‚    storage           â”‚                    â”‚  (2-3 seconds)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                                                       â–¼
                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                            â”‚  6. Format Response      â”‚
                                            â”‚                          â”‚
                                            â”‚  Returns:                â”‚
                                            â”‚  â€¢ Answer text           â”‚
                                            â”‚  â€¢ Source pages          â”‚
                                            â”‚  â€¢ Content types         â”‚
                                            â”‚  â€¢ Response time         â”‚
                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                                                       â–¼
                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                            â”‚    USER RECEIVES:        â”‚
                                            â”‚                          â”‚
                                            â”‚  "The EX-1280C has 12    â”‚
                                            â”‚   analog inputs..."      â”‚
                                            â”‚                          â”‚
                                            â”‚  Sources:                â”‚
                                            â”‚  â€¢ Page 12 (spec)        â”‚
                                            â”‚  â€¢ Page 45 (spec)        â”‚
                                            â”‚                          â”‚
                                            â”‚  Time: 2.34s             â”‚
                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Processing Flow in Detail

**Phase 1: Document Setup (Run Once)**
```
User places PDFs in data/ folder
    â†“
Runs: python scripts/demo.py
    â†“
For each PDF:
    1. Router selects extraction method (text/table/OCR)
    2. Content detector classifies chunks
    3. Text splitter creates semantic segments
    4. Embedder converts text â†’ vectors
    5. ChromaDB stores vectors + metadata
    â†“
Database ready: 156 chunks stored
```

**Phase 2: Query Handling (Every Question)**
```
User types: "What is the warranty period?"
    â†“
Frontend â†’ POST /api/query â†’ FastAPI
    â†“
RAG Orchestrator:
    1. Embedder: Question â†’ vector
    2. ChromaDB: Find 5 most similar chunks
    3. Retriever: Get full Document objects
    4. Prompt Builder: Detect intent, build prompt
    5. Ollama: Send prompt â†’ Phi-2 â†’ generate answer
    6. Format: Add sources, timing
    â†“
Response â†’ Frontend â†’ Display to user
    â†“
Total time: 2-3 seconds
```

---

## Technical Approach & Design Decisions

### 1. Why RAG (Retrieval-Augmented Generation)?

**The Challenge:** LLMs don't "know" your specific documents
**The Solution:** RAG combines:
- **Retrieval**: Find relevant information from your documents
- **Generation**: Use LLM to synthesize natural language answers

**Why not just use ChatGPT?**
- âŒ Can't access private/internal documents
- âŒ May hallucinate (make up information)
- âŒ No source citations
- âŒ Ongoing API costs
- âŒ Data privacy concerns

**Why RAG wins:**
- âœ… 100% based on your documents
- âœ… Source citations for every answer
- âœ… Works offline
- âœ… No ongoing costs
- âœ… Complete data privacy

### 2. Technology Stack Rationale

| Component | Choice | Why? | Alternatives Considered |
|-----------|--------|------|------------------------|
| **LLM** | Phi-2 (via Ollama) | â€¢ 1.6GB (fits 8GB RAM target)<br>â€¢ 2-3s response time<br>â€¢ Runs locally<br>â€¢ No API costs | GPT-4 (expensive, cloud), Llama (requires 8GB+) |
| **Embeddings** | all-MiniLM-L6-v2 | â€¢ Lightweight (80MB)<br>â€¢ Fast inference<br>â€¢ Good quality<br>â€¢ Sentence Transformers | OpenAI Ada (costs), BGE-large (slower) |
| **Vector DB** | ChromaDB | â€¢ Simple Python API<br>â€¢ Persistent storage<br>â€¢ No separate server<br>â€¢ Built for embeddings | Pinecone (cloud), Qdrant (overkill), FAISS (no persistence) |
| **Web Framework** | FastAPI | â€¢ Async support<br>â€¢ Auto API docs<br>â€¢ Fast performance<br>â€¢ Modern | Flask (older, slower), Django (too heavy) |
| **PDF Processing** | PyPDF2 + Camelot + Tesseract | â€¢ Multi-strategy<br>â€¢ Handles all PDF types<br>â€¢ Table extraction<br>â€¢ OCR fallback | PyMuPDF (limited tables), pdfplumber (slower) |

### 3. Key Design Decisions

#### Decision 1: Local-First Architecture
**Choice:** Everything runs locally (Ollama, embeddings, vector DB)

**Reasoning:**
- Privacy: Documents never leave the machine
- No API costs or rate limits
- Works offline
- Predictable performance

**Trade-off:** Requires local compute (8GB RAM minimum)

#### Decision 2: Persistent Vector Store
**Choice:** ChromaDB with disk persistence (not in-memory)

**Reasoning:**
- Survive application restarts
- No need to reprocess documents
- Fast startup time

**Implementation:**
```python
# In chromadb_manager.py
self.client = chromadb.PersistentClient(path=config.VECTOR_DB_DIR)
```

#### Decision 3: Multi-Strategy PDF Processing
**Choice:** Try multiple extraction methods per document

**Reasoning:**
- PDFs vary wildly in structure
- Text extraction works for most
- Tables need special handling
- Scanned docs need OCR

**Implementation:**
```python
# In router.py
1. Try text extraction (fast)
2. If tables detected â†’ use Camelot
3. If text extraction fails â†’ OCR with Tesseract
```

#### Decision 4: Content-Aware Prompting
**Choice:** Classify content type, adjust prompts accordingly

**Reasoning:**
- Specifications need factual, precise answers
- Procedures need step-by-step instructions
- Different content = different prompt styles

**Implementation:**
```python
# In prompt_builder.py
if content_type == "specification":
    prompt = "Provide exact specifications from the documents..."
elif content_type == "procedure":
    prompt = "Provide step-by-step instructions..."
```

#### Decision 5: Semantic Chunking
**Choice:** 800-character chunks with 100-character overlap

**Reasoning:**
- 800 chars = ~1-2 paragraphs (semantic unit)
- 100-char overlap preserves context across boundaries
- Fits in embedding model's token limit

**Trade-offs tested:**
- Too small (400): Lost context
- Too large (1500): Diluted relevance
- No overlap: Missed boundary information

### 4. Performance Optimization

**Target:** 8GB RAM, 2-3 second response time

**Optimizations applied:**
1. **Model Selection:** Phi-2 (1.6GB) instead of Llama-7B (7GB)
2. **Embedding Cache:** HuggingFace caches models locally
3. **Batch Processing:** ChromaDB batch inserts during ingestion
4. **Async API:** FastAPI async endpoints for concurrency
5. **Persistent DB:** No reprocessing on restart

**Measured Performance:**
- Cold start: ~5 seconds (load models)
- Warm query: 2-3 seconds average
- Memory idle: ~500MB
- Memory during query: ~1.5GB
- Concurrent users: 100+ (async)

### 5. Error Handling Strategy

**Philosophy:** Graceful degradation, never crash

**Implementation:**
```python
# Three-layer error handling:
1. Try-except at component level (specific recovery)
2. ErrorHandler class for logging and categorization
3. User-friendly messages in API responses
```

**Error types handled:**
- Ollama not running â†’ Clear error message
- No documents loaded â†’ Instruction to process docs
- No relevant context â†’ Suggest rephrasing question
- Timeout â†’ Configurable RESPONSE_TIMEOUT

---

## Quick Start Guide

### Prerequisites

1. **Python 3.11+** ([Download](https://www.python.org/downloads/))
2. **Ollama** with Phi model:
   ```bash
   # Install Ollama: https://ollama.ai
   ollama pull phi
   ```

### Installation & Setup

```powershell
# 1. Clone repository
git clone https://github.com/Munna-Git/bose-rag-project.git
cd bose-rag-project

# 2. Create virtual environment
python -m venv venv
.\venv\Scripts\Activate.ps1

# 3. Install dependencies
pip install -r requirements.txt
pip install fastapi uvicorn

# 4. Process documents (one-time setup)
#    Place your PDFs in the data/ folder first
python scripts\demo.py

# 5. Launch application
.\launch_fastapi.bat
```

**Access:** http://localhost:8000

---

## Usage Examples

### Web Interface

1. Open http://localhost:8000
2. Wait for green "Online" status
3. Type question: *"What is the power consumption?"*
4. Receive answer with page citations in ~2 seconds

### REST API

```bash
# Submit query
curl -X POST "http://localhost:8000/api/query" \
  -H "Content-Type: application/json" \
  -d '{"question": "What are the dimensions?"}'

# Check health
curl http://localhost:8000/api/health

# Get system info
curl http://localhost:8000/api/info
```

### Python API

```python
from src.interfaces.rag_phi import BoseRAGPhi

# Initialize
rag = BoseRAGPhi()

# Process documents (one-time)
result = rag.process_documents([
    "data/manual.pdf",
    "data/spec_sheet.pdf"
])
print(f"Processed {result['total_chunks']} chunks")

# Query
response = rag.answer_query("What is the warranty period?")
print(f"Answer: {response['answer']}")
print(f"Sources: {response['sources']}")
print(f"Time: {response['time']}")
```

---

## Configuration

### Environment Variables

Create `.env` file:
```env
# Model Configuration
OLLAMA_BASE_URL=http://localhost:11434
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Processing
CHUNK_SIZE=800
CHUNK_OVERLAP=100

# Generation
MAX_TOKENS=512
TEMPERATURE=0.1

# System
LOG_LEVEL=INFO
VECTOR_DB_DIR=data/vector_db
```

### Customization

**Change UI theme** (`static/styles.css`):
```css
:root {
    --bose-accent: #00a0dc;  /* Your brand color */
}
```

**Adjust retrieval count** (`src/interfaces/rag_phi.py`):
```python
docs = self.retriever.retrieve(query, k=5)  # Change k
```

---

## Monitoring & Maintenance

### Check Database
```powershell
python scripts\check_db.py
```

Output:
```
Database Status:
Total documents: 156
Sample query results: Top 3 chunks with distances
```

### Clear Database
```powershell
python scripts\clear_db.py
```

### View Logs
```powershell
Get-Content rag_system.log -Tail 50 -Wait
```

### Backup
```powershell
Copy-Item -Recurse data\vector_db data\vector_db_backup_$(Get-Date -Format 'yyyyMMdd')
```

---

## Deployment Options

### Development
```powershell
python app.py
```

### Production (Windows Service)
```powershell
# Install NSSM
choco install nssm

# Create service
nssm install BoseRAGAPI "D:\bose-rag-project\venv\Scripts\python.exe" "app.py"
nssm set BoseRAGAPI AppDirectory "D:\bose-rag-project"
nssm start BoseRAGAPI
```

### Docker
```bash
docker build -t bose-rag .
docker run -p 8000:8000 bose-rag
```

See [DEPLOYMENT.md](DEPLOYMENT.md) for cloud deployment (AWS, Azure, GCP).

---

## For Interviewers: Key Highlights

### What This Project Demonstrates

1. **Full-Stack AI/ML Application**
   - Backend: FastAPI (Python)
   - Frontend: HTML/CSS/JavaScript
   - AI: LangChain + Ollama + ChromaDB
   - Integration: REST API, async processing

2. **Production-Ready Engineering**
   - Modular architecture (separation of concerns)
   - Comprehensive error handling
   - Persistent storage
   - Structured logging
   - Configuration management
   - Deployment automation

3. **AI/ML Best Practices**
   - RAG architecture (retrieval + generation)
   - Vector embeddings for semantic search
   - Multi-strategy PDF processing
   - Content-aware prompting
   - Local LLM integration

4. **User-Centric Design**
   - Professional branded UI
   - Real-time status feedback
   - Source citations for trust
   - Error messages guide users
   - Responsive design

5. **System Design Thinking**
   - Resource constraints (8GB target)
   - Performance optimization (2-3s responses)
   - Scalability considerations (async, stateless)
   - Privacy-first (local processing)

### Technical Decisions Worth Discussing

- **Why Phi-2?** Balances performance vs resource usage (1.6GB)
- **Why ChromaDB?** Simple, persistent, no separate server needed
- **Why multi-strategy PDF?** Real-world PDFs vary wildly in structure
- **Why content-aware prompts?** Different content needs different framing
- **Why persistent storage?** Avoid reprocessing, faster startup

### Potential Improvements

1. **Caching:** Redis for frequent queries
2. **GPU Acceleration:** 10x faster with CUDA
3. **Hybrid Search:** Combine semantic + keyword search
4. **Query Rewriting:** Expand abbreviations, fix typos
5. **Answer Verification:** Cross-reference multiple chunks
6. **User Feedback Loop:** Rating system to improve responses
7. **Multi-Language:** Support non-English documents
8. **Streaming Responses:** Show answer as it generates

---

## Future Enhancements (Roadmap)

This section outlines a pragmatic, phased roadmap that balances user impact, system robustness, and strategic scalability. Interview discussion ready.

## Short-Term Priorities
- Add user feedback (thumbs up/down with optional comment).  
- Implement basic caching and query normalization (lowercasing, unit normalization, abbreviation expansion).  
- Provide expandable source citations.  
- Add simple API rate limiting.

---

## Medium-Term Improvements
- Hybrid retrieval (vector + BM25) for better accuracy, especially on numeric/spec queries.  
- Query rewriting to improve ambiguous or shorthand user input.  
- Confidence validation when retrieved chunks conflict.  
- Adaptive chunking (smaller for dense tables, larger for narrative sections).  
- Observability dashboard displaying metrics like latency, cache hits, and error categories.

---

## Long-Term Vision
- Multi-tenancy with role-based access control (RBAC).  
- Integrate structured metadata (CSV/DB) into retrieval alongside unstructured text.  
- Fine-tune a small domain model and add multilingual query support.  
- Deploy lightweight retrieval and inference models for edge-based use cases.

---

## User Experience Enhancements
- Preserve tables in citations for readability.  
- Suggest relevant follow-up questions after each result.  
- Optional short-term conversation memory with controlled session scope.  
- Allow exporting results and citations to a downloadable PDF.

---

## License

MIT License - Free to use, modify, distribute.

---

## ğŸ“ Support & Documentation

- **Full Deployment Guide:** [DEPLOYMENT.md](DEPLOYMENT.md)
- **FastAPI Details:** [FASTAPI_README.md](FASTAPI_README.md)
- **API Documentation:** http://localhost:8000/docs (when running)
- **Logs:** `rag_system.log`
- **Database Check:** `python scripts\check_db.py`

---

<div align="center">

**Built with â¤ï¸ for Bose Professional Technical Support**

*Demonstrating production-ready AI/ML engineering*

[GitHub](https://github.com/Munna-Git/bose-rag-project) â€¢ [Issues](https://github.com/Munna-Git/bose-rag-project/issues) â€¢ [Documentation](DEPLOYMENT.md)

</div>
